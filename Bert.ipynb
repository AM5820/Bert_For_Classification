{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "F_E1W3cwv7on"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import names\n",
    "import csv\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V1y8F5RbYc8C"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I1zemG4tYc-9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "h13UB3THZUU1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SiAZK6T2GAjc"
   },
   "outputs": [],
   "source": [
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:                    \n",
    "        return None\n",
    "def lemmatize_sentence(sentence):\n",
    "    lem = WordNetLemmatizer()\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:                        \n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lem.lemmatize(word, tag))\n",
    "    return \" \".join(res_words)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'http\\S+','',tweet)        # remove urls\n",
    "    tweet = re.sub('[^a-z]',' ',tweet)\n",
    "    tweet = lemmatize_sentence(tweet)\n",
    "\n",
    "    tweet = word_tokenize(tweet)\n",
    "    add_stop = ['religion','religious']\n",
    "\n",
    "    #tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
    "    tweet = [word for word in tweet if not word in set(stopwords.words('english'))]\n",
    "    tweet = [word for word in tweet if not word in set(names.words()) and not word in set(add_stop)]\n",
    "\n",
    "    tweet = \" \".join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "TBoVBrANwCNx",
    "outputId": "33859243-8ff1-4b4a-b6e5-f45187921cc8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7174b1e7-7d4d-41b5-99df-72bc8743a050\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9789</th>\n",
       "      <td>Sneak peek! https://t.co/rwIwMQZ8Ac</td>\n",
       "      <td>celebrity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20152</th>\n",
       "      <td>Another great zSpace customer getting started....</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18771</th>\n",
       "      <td>@wildboydayo with verifiable facts &amp;amp; not o...</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7174b1e7-7d4d-41b5-99df-72bc8743a050')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7174b1e7-7d4d-41b5-99df-72bc8743a050 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7174b1e7-7d4d-41b5-99df-72bc8743a050');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                 content      topic\n",
       "9789                 Sneak peek! https://t.co/rwIwMQZ8Ac  celebrity\n",
       "20152  Another great zSpace customer getting started....  education\n",
       "18771  @wildboydayo with verifiable facts &amp; not o...  education"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle('final_data_manual.pickle')\n",
    "dataset.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "IHRzp8mEi3TW"
   },
   "outputs": [],
   "source": [
    "cel_df = dataset[dataset.topic == 'celebrity'].sample(3000)\n",
    "health_df = dataset[dataset.topic == 'health'].sample(3000)\n",
    "sport_df = dataset[dataset.topic == 'sports'].sample(3000)\n",
    "pol_df = dataset[dataset.topic == 'politics'].sample(3000)\n",
    "rel_df = dataset[dataset.topic == 'religion'].sample(3000)\n",
    "edu_df = dataset[dataset.topic == 'education'].sample(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "FAqJZDNTiwwX"
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(np.row_stack([cel_df,health_df,sport_df,pol_df,rel_df,edu_df]),columns=['content','topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "GKNTlVuSi0Ep"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(dataset.content,dataset.topic,test_size=0.2,stratify=dataset.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "kQ0QX7Pri4gv"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(np.column_stack([x_train,y_train]),columns=['content','topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "mg1oXwLUi-bz"
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(np.column_stack([x_test,y_test]),columns=['content','topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZesT3RmGpyZ",
    "outputId": "13124649-d55a-4e8d-bfe6-910192c2d650"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "health       2400\n",
       "religion     2400\n",
       "education    2400\n",
       "celebrity    2400\n",
       "politics     2400\n",
       "sports       2400\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeJpOG_qrwKW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "k4HqOEZPkATj"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "xH1Ggk7jZWai"
   },
   "outputs": [],
   "source": [
    "# the model we gonna train, base uncased BERT\n",
    "# check text classification models here: https://huggingface.co/models?filter=text-classification\n",
    "model_name = \"bert-base-uncased\"\n",
    "# max sequence length for each document/sentence sample\n",
    "max_length = 512\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "\n",
    "# tokenize the dataset, truncate when passed `max_length`, \n",
    "# and pad with 0's when less than `max_length`\n",
    "train_encodings = tokenizer(train_df.content.to_list(), truncation=True, padding=True, max_length=max_length)\n",
    "valid_encodings = tokenizer(test_df.content.to_list(), truncation=True, padding=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "OeZQnF0sZjrj"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_train = LabelEncoder()\n",
    "lb_val = LabelEncoder()\n",
    "\n",
    "train_labels =  lb_train.fit_transform(train_df.topic)\n",
    "val_labels =  lb_val.fit_transform(test_df.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "8y7yGMoyXYJd"
   },
   "outputs": [],
   "source": [
    "label_names = pd.DataFrame({'encode':train_labels,'true':train_df.topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "59Az632SX_bW"
   },
   "outputs": [],
   "source": [
    "target_names = {}\n",
    "for i,k in zip(label_names.encode,label_names.true):\n",
    "  target_names[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNw9HIMzY9f2",
    "outputId": "a6e7d360-78bf-40e4-efab-199e2e6ce06e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'celebrity',\n",
       " 1: 'education',\n",
       " 2: 'health',\n",
       " 3: 'politics',\n",
       " 4: 'religion',\n",
       " 5: 'sports'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "EbeBCMsuZaB2"
   },
   "outputs": [],
   "source": [
    "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = NewsGroupsDataset(train_encodings, train_labels)\n",
    "valid_dataset = NewsGroupsDataset(valid_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Uw2ZYYIZcnU"
   },
   "outputs": [],
   "source": [
    "# load the model and pass to CUDA\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(target_names)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "osyDus4RZoYH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "C9a9QpYoZwGC"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "    warmup_steps=3000,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    logging_steps=400,               # log & save weights each logging_steps\n",
    "    save_steps=400,\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "SgDrFyvSZxWg"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "bWDu-iULZyqz",
    "outputId": "ed542012-663e-4ba4-c00f-f2f8babf0088"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 14400\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5237' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5237/5400 1:00:45 < 01:53, 1.44 it/s, Epoch 2.91/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.458200</td>\n",
       "      <td>0.554270</td>\n",
       "      <td>0.945556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.065807</td>\n",
       "      <td>0.985833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.060735</td>\n",
       "      <td>0.988611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.071266</td>\n",
       "      <td>0.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.077076</td>\n",
       "      <td>0.985278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.064503</td>\n",
       "      <td>0.989722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.065763</td>\n",
       "      <td>0.989167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.169483</td>\n",
       "      <td>0.969167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.079144</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.074785</td>\n",
       "      <td>0.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.055791</td>\n",
       "      <td>0.990833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.044517</td>\n",
       "      <td>0.991944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.992500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-400\n",
      "Configuration saved in ./results/checkpoint-400/config.json\n",
      "Model weights saved in ./results/checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-800\n",
      "Configuration saved in ./results/checkpoint-800/config.json\n",
      "Model weights saved in ./results/checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-1200\n",
      "Configuration saved in ./results/checkpoint-1200/config.json\n",
      "Model weights saved in ./results/checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-1600\n",
      "Configuration saved in ./results/checkpoint-1600/config.json\n",
      "Model weights saved in ./results/checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-2400\n",
      "Configuration saved in ./results/checkpoint-2400/config.json\n",
      "Model weights saved in ./results/checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-2800\n",
      "Configuration saved in ./results/checkpoint-2800/config.json\n",
      "Model weights saved in ./results/checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-3200\n",
      "Configuration saved in ./results/checkpoint-3200/config.json\n",
      "Model weights saved in ./results/checkpoint-3200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-3600\n",
      "Configuration saved in ./results/checkpoint-3600/config.json\n",
      "Model weights saved in ./results/checkpoint-3600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-4400\n",
      "Configuration saved in ./results/checkpoint-4400/config.json\n",
      "Model weights saved in ./results/checkpoint-4400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-4800\n",
      "Configuration saved in ./results/checkpoint-4800/config.json\n",
      "Model weights saved in ./results/checkpoint-4800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to ./results/checkpoint-5200\n",
      "Configuration saved in ./results/checkpoint-5200/config.json\n",
      "Model weights saved in ./results/checkpoint-5200/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5400' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5400/5400 1:02:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.458200</td>\n",
       "      <td>0.554270</td>\n",
       "      <td>0.945556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.065807</td>\n",
       "      <td>0.985833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.060735</td>\n",
       "      <td>0.988611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.071266</td>\n",
       "      <td>0.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.077076</td>\n",
       "      <td>0.985278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.064503</td>\n",
       "      <td>0.989722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.065763</td>\n",
       "      <td>0.989167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.169483</td>\n",
       "      <td>0.969167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.079144</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.074785</td>\n",
       "      <td>0.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.055791</td>\n",
       "      <td>0.990833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.044517</td>\n",
       "      <td>0.991944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.992500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-5200 (score: 0.042330753058195114).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5400, training_loss=0.17044621310852193, metrics={'train_runtime': 3729.3194, 'train_samples_per_second': 11.584, 'train_steps_per_second': 1.448, 'total_flos': 7725875823129600.0, 'train_loss': 0.17044621310852193, 'epoch': 3.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgrUTZIGhYpX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "9veE-auwbMbb",
    "outputId": "82f74bf9-e04e-42d1-87fa-7e57a6335ace"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3600\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 3.0,\n",
       " 'eval_accuracy': 0.9925,\n",
       " 'eval_loss': 0.042330753058195114,\n",
       " 'eval_runtime': 73.0285,\n",
       " 'eval_samples_per_second': 49.296,\n",
       " 'eval_steps_per_second': 2.465}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UJnz_UagiXx"
   },
   "outputs": [],
   "source": [
    "# saving the fine tuned model & tokenizer\n",
    "model_path = \"20newsgroups-bert-base-uncased\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "AVZ9UBWyhAoj"
   },
   "outputs": [],
   "source": [
    "def get_prediction(text):\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    # get output probabilities by doing softmaxs\n",
    "    probs = outputs[0].softmax(1)\n",
    "    # executing argmax function to get the candidate label\n",
    "    return target_names[probs.argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grUj1kk-hSu5",
    "outputId": "7e94c083-d83f-4bbd-d2ca-e1bb63eb4f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celebrity\n"
     ]
    }
   ],
   "source": [
    "# Example #1\n",
    "text = \"\"\" johnny depp and his wife amber herd at the court \"\"\"\n",
    "print(get_prediction(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Iof1YOanuVVd"
   },
   "outputs": [],
   "source": [
    "def get_prediction(text):\n",
    "    max_length = 512\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    # get output probabilities by doing softmaxs\n",
    "    probs = outputs[0].softmax(1)\n",
    "    # executing argmax function to get the candidate label\n",
    "    return target_names[probs.argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_model/target_names.pickle','rb') as f:\n",
    "    target_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xde2hcfCqkXt"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert_model/',local_files_only=True)\n",
    "model = BertForSequenceClassification.from_pretrained('bert_model/pytorch_model.bin',config='bert_model/config.json', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6kTUcG1uWEr",
    "outputId": "521bc379-1cb0-46ed-e7c5-6bb73bd311c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Johnny Depp victorious in defamation case against ex-wife Amber Heard  :  celebrity\n",
      "------------\n",
      " schools are places for mental thinking  :  education\n",
      "------------\n",
      " disases caused too many people to die  :  health\n",
      "------------\n",
      " Putin announced a \"special military operation\" to \"demilitarise and denazify\" Ukraine  :  politics\n",
      "----------\n",
      " Religious belief usually relates to faith in divine involvement in the universe and human life  :  religion\n",
      "------------\n",
      " The top-2 teams take on each other for the UAAP Season 84 Women's Volleyball title... fitting  :  sports\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example #1\n",
    "tweet = \"\"\" Johnny Depp victorious in defamation case against ex-wife Amber Heard \"\"\"\n",
    "text = preprocess_tweet(tweet)\n",
    "print(f'{tweet} :  {get_prediction(text)}\\n------------')\n",
    "\n",
    "# Example #2\n",
    "tweet = \"\"\" schools are places for mental thinking \"\"\"\n",
    "text = preprocess_tweet(tweet)\n",
    "print(f'{tweet} :  {get_prediction(text)}\\n------------')\n",
    "\n",
    "# Example #3\n",
    "tweet = \"\"\" disases caused too many people to die \"\"\"\n",
    "text = preprocess_tweet(tweet)\n",
    "print(f'{tweet} :  {get_prediction(text)}\\n------------')\n",
    "\n",
    "# Example #4\n",
    "tweet = \"\"\" Putin announced a \"special military operation\" to \"demilitarise and denazify\" Ukraine \"\"\"\n",
    "text = preprocess_tweet(tweet)\n",
    "print(f'{tweet} :  {get_prediction(text)}\\n----------')\n",
    "\n",
    "# Example #5\n",
    "tweet = \"\"\" Religious belief usually relates to faith in divine involvement in the universe and human life \"\"\"\n",
    "text = preprocess_tweet(tweet)\n",
    "print(f'{tweet} :  {get_prediction(text)}\\n------------')\n",
    "\n",
    "# Example #6\n",
    "tweet = \"\"\" The top-2 teams take on each other for the UAAP Season 84 Women's Volleyball title... fitting \"\"\"\n",
    "text = preprocess_tweet(tweet)\n",
    "print(f'{tweet} :  {get_prediction(text)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "eT9qzPKuuYM6"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "yOmWjqVuuWeJ"
   },
   "outputs": [],
   "source": [
    "with open('target_names.pickle','wb') as f:\n",
    "  pickle.dump(target_names,f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "ds7SFl_RyAkd"
   },
   "outputs": [],
   "source": [
    "with open('target_names.pickle','rb') as f:\n",
    "  p = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70WxT3LIyApZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kChSxR7fy8YR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bert",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
